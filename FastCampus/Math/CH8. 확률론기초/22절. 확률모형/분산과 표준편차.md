# 분산과 표준 편차

### 확률 분포의 분산

확률 밀도 함수 f(x)의 수식을 알고 있다면 다음처럼 이론적인 분산을 구할 수 있다.
분산을 구하는 연산자는 영어 Variance를 따서 $$Var[.]$$ 로 표기하고 이 연산자로 계산된 분산값은 $$\sigma^2$$ 으로 표기한다.

$$\sigma^2 = Var[X] = E[(X-\mu)^2]$$

</br>



이산 확률 변수의 경우에는 확률 질량 함수 $$P(x)$$ 를 사용하여 분산을 구한다.

$$\sigma^2 = Var[X] = E[(X-\mu)^2] = \sum_{x_i\in\Omega}(x-\mu)^2P(x_i)$$

</br>



이산 확률 변수의 경우에는 확률 밀도 함수 $$f(x)$$ 를 사용하여 분산을 구한다.

$$\sigma^2 = Var[X] = E[(X-\mu)^2] = \int_{-\infty}^\infty(x-\mu)^2f(x)dx$$



즉, 분산은 평균으로부터 데이터까지의 거리 제곱을 확률 $$P(x)$$ 또는 확률 밀도 $$f(x)$$ 를 가중치로 하여 평균한 것으로 볼 수 있다.



### 분산의 성질

분산은 다음과 같은 성질을 만족한다.

- 0 또는 양수

$$Var[X] \geq 0$$

</br>



- 랜덤 변수가 아닌 상수 값 $$c$$ 에 대해 

$$Var[cX] = c^2Var[X]$$

또한 기댓값의 성질을 이용하여 다음 성질을 증명할 수 있다.

$$Var[X] = E[X^2] - (E[X])^2 = E[X^2] - \mu^2$$

$$E[X^2] = \mu^2 + Var[X]$$



 ### 두 확률 변수의 합의 분산

두 확률 변수 $$X, Y$$ 의 합의 분산은 각 확률 변수의 분산의 합과 다음과 같은 관계가 있다. 마지막 항은 양수도 될 수 있고, 음수도 될 수 있다.

$$Var[X+Y] = Var[X]+Var[Y]+2E[(X-\mu_X)(Y-\mu_y)]$$

</br>



그런데 두 확률 변수 $$X, Y$$ 가 서로 독립이면 다음 식이 성립한다. 확률 변수가 독립이라는 것은 서로 영향을 미치지 않는 것을 의미하며 확률 변수의 독립의 수학적 정의와 왜 다음 식이 성립하는가는 추후 설명하기로 한다.

$$E[(X-\mu_X)(Y-\mu_Y)]=0$$

위 식을 이용하면 독립인 두 확률 변수의 합의 분산은 분산의 합과 같다는 것을 보일 수 있다.

$$Var[X+Y] = Var[X]+Var[Y]$$



### 샘플 평균의 분산

확률 변수 $$X$$ 의 샘플 평균 $$\bar X$$도 일종의 확률 변수이고 그 기댓값 $$E[\bar X]$$ 은 원래 확률 변수 $$X$$ 의 기댓값 $$E[X]$$ 과 일치한다는 것을 기억하자.

$$E[\bar X] = E[X]$$

샘플 평균 $$\bar X$$ 의 분산 $$Var[\bar X]$$ 은 원래 확률 변수 $$X$$ 의 분산 $$Var[X]$$ 과 다음 관계를 가진다.

$$Var[\bar X] = 1/NVar[X]$$

따라서 샘플 평균을 취하는 샘플의 수가 커지면 샘플 평균의 값은 변동이 적어진다. 
샘플의 수가 무한대로 다가가면 샘플 평균의 값은 항상 일정한 값이 나온다.

위 식이 의미하는 바는 다음과 같다.

- 데이터를 생성하는 확률 변수의 기댓값을 구하려면 확률 밀도 함수의 수식을 알아야 한다.
- 그런데 우리는 데이터를 생성하는 확률 변수의 확률 밀도 함수의 수식을 정확히 알지 못한다.
- 하지만 샘플 평균이라는 새로운 확률 변수의 기댓값은 원래 확률 변수의 기댓값과 같으므로 이 값을 알면 된다.
- 만약 샘플의 갯수가 크면 샘플 평균의 분산이 아주 작아지므로 샘플 평균의 샘플 값과 샘플 평균의 기댓값은 거의 같은 값이다.
- 따라서 샘플 평균의 기댓값을 구하면 원래 확률 변수의 기댓값의 근사값을 구할 수 있다.



### 샘플 분산의 기댓값

앞에서 샘플 평균의 기댓값을 구하면 이론적인 평균 즉, 기댓값과 같아진다는 것을 알았다.

그런데, 샘플 분산 $$S^2$$ 의 기댓값을 구하면 이론적인 분산 $$\sigma^2$$ 과 같아지는 것이 아니라 이론적인 분산값의 $$\dfrac{N-1}{N}$$ 이 된다. 즉, 작아진다.

$$E[S^2] = \dfrac{N-1}{N}\sigma^2$$

</br>



그러므로 샘플 분산의 기댓값이 정확하게 $$\sigma^2$$ 이 되려면 거리 제곱의 평균을 구할 때 분모가 $$N$$ 이 아니라 $$N-1$$ 이 되어야 한다.

$$\sigma^2$$

= $$\dfrac{N}{N-1} \text{E}[S^2]$$

= $$\dfrac{N}{N-1} \text{E} \left[ \dfrac{1}{N} \sum (X_i-\bar{X})^2 \right]$$

= $$\text{E} \left[ \dfrac{1}{N-1} \sum (X_i-\bar{X})^2 \right] $$

= $$\text{E} \left[ S^2_{\text{unbiased}} \right]$$





